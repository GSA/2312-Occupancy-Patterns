---
title: "Data preparation"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    highlight: tango
    code_folding: hide
params:
  author: "Ben Jaques-Leslie"
  project_number: 2312
  project_name: "Occupancy Patterns"
  google_drive_home: "G:/Shared drives/MSG Projects/1.0 Real Estate Solutions (LQ1)/2312 Occupancy Patterns"
  data_folder_1: "Rosters"
  unique_identifier_1: !r c("snapshot_date","employee_e_mail_address")
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("arsenal")
# devtools::install_github(repo = "GSA/oesrrr")
library(oesrrr)
library(tidyverse)
library(janitor)
library(skimr)
library(DataExplorer)
library(readr)
library(arsenal)
library(Hmisc)
library(readxl)
```

# Data preparation {.tabset .tabset-pills}

**Project number**: `r params$project_number`

**Project name**: `r params$project_name`

**Author**: `r params$author`

**Data folder 1**: `r params$data_folder_`'

**Unique identifier for data folder 1**: `r params$unique_identifier_1`

## Functions

Create function to drop rows with null unique identifiers.

```{r}
filter_unique <- function(in_data, ...)
{
  in_data %>% 
  filter(!(if_any(c(...),~is.na(.))))
}
```

## Load data {.tabset .tabset-pills}

### Data from `r params$data_folder_1`

Find the list of .csv files in the `r params$data_folder_1` data folder.

```{r}
files <- 
  list.files(file.path(params$google_drive_home,"03. Data Collection",params$data_folder_1), pattern = ".xlsx", full.names = TRUE)
files
```

Load all data files in the `r params$data_folder_1` data folder.

```{r}
prep_base <- files %>% 
  map_dfr(~mutate(.data = read_xlsx(path = ., 
                                    col_types = c("date", "text", "text", 
        "text", "text", "text", "text", "text", 
        "text", "date", "text", "text", "text", 
        "text", "text", "text", "text", "text", 
        "text", "text", "text", "text", "text", 
        "text", "text", "text", "text", "text")), 
        file_name = str_remove(.,pattern = file.path(params$google_drive_home,"03. Data Collection"))
      )) %>% 
  clean_names() %>% 
  distinct()
```

### Review data

```{r}
prep_base %>% skim()
```

## Filter null unique ids

```{r}
prep_base <- 
  prep_base %>% 
  filter_unique(params$unique_identifier_1)
```

### Review data

```{r}
prep_base %>% skim()
```

### Identifying and categorizing columns

Created a dataframe of variable metadata

```{r}
column_meta_data <- 
  prep_base %>% 
  contents()

column_meta_data <- 
  column_meta_data$contents %>% 
  as.data.frame()

column_meta_data <-
  column_meta_data %>% 
  rownames_to_column(var = "variable")

column_meta_data
```

## Addressing duplication

We are using `r params$unique_identifier_1` as the unique identifier. 

### Check for duplication

```{r}
dupe_01 <- 
  prep_base %>%
  get_dupes(params$unique_identifier_1)
```

#### Data review

```{r}
dupe_01 %>% 
  skim()
```

No duplication found.

## Addressing blanks

Function that checks whether there are duplicates for different variables

```{r}
blank_check <- function(in_data,in_variable_name)
{
  if(prep_base %>% select(!!in_variable_name) %>% pull() %>% class() %>% head(1) == "POSIXct")
    {
    out <- FALSE
  }
  
  if(prep_base %>% select(!!in_variable_name) %>% pull() %>% class() %>% head(1) != "POSIXct")
   {
      check_01 <- 
  in_data %>% 
    filter(!!sym(in_variable_name) == "") %>% 
  distinct()

out <- nrow(check_01) > 0 
   }
 

return(out)
}
```

Checking duplicates for each variable in the dataframe.

```{r}
blanks_by_variable <- 
  column_meta_data %>% 
  pull(variable) %>% 
  map_lgl(~blank_check(in_data = prep_base,.))

blanks_by_variable
```

Add this to the `column_meta_data` dataframe. 

```{r}
column_meta_data <- 
  column_meta_data %>% 
  add_column(
    blanks_by_variable = blanks_by_variable
  )

column_meta_data
```

Create dataframe of only variables that have blanks. 

```{r}
blanking_variables <- 
  column_meta_data %>% 
  filter(blanks_by_variable)

blanking_variables
```

No variables with blank entries. 

## Addressing nulls

Function that checks whether there are duplicates for different variables

```{r}
null_check <- function(in_data,in_variable_name)
{
  check_01 <- 
  in_data %>% 
    filter(is.na(!!sym(in_variable_name))) %>% 
  distinct()

out <- nrow(check_01) > 0 

return(out)
}
```

Checking duplicates for each variable in the dataframe.

```{r}
nulls_by_variable <- 
  column_meta_data %>% 
  pull(variable) %>% 
  map_lgl(~null_check(in_data = prep_base,.))

nulls_by_variable
```

Add this to the `column_meta_data` dataframe. 

```{r}
column_meta_data <- 
  column_meta_data %>% 
  add_column(
    nulls_by_variable = nulls_by_variable
  )

column_meta_data
```

Create dataframe of only variables that have blanks. 

```{r}
null_variables <- 
  column_meta_data %>% 
  filter(nulls_by_variable)

null_variables
```

Grouping variables.

```{r}
null_variables <- 
  null_variables %>% 
  mutate(group = 
           case_when(
             variable == "supervisor_display_name" ~ "supervisor_display_name",
             variable == "supervisor_e_mail_address" ~ "supervisor_display_name",
             variable == "business_line_2_letter_department_id" ~ "business_line_2_letter_department_id",
             variable == "location_city" ~ "business_line_2_letter_department_id",
             variable == "location_state" ~ "business_line_2_letter_department_id",
             variable == "position_category_code" ~ "position_category_code",
             variable == "position_category_description" ~ "position_category_code",
             TRUE ~ variable
             )
         )

null_variables
```

Create list of groups.

```{r}
null_groups <- 
  null_variables %>% 
  select(group) %>% 
  distinct() %>% 
  pull()

null_groups
```

### Nulls removal

```{r}
step_n <- 0
```

```{r}
step_n <- step_n + 1
```

#### Step `r step_n`

Removing nulls produced by:

```{r}
null_variable <- 
  null_variables %>% 
  filter(group == null_groups[step_n])

null_variable
```

Reviewing blanks.

```{r}
null_01 <- 
  prep_base %>% 
  filter(if_any(all_of(null_variable %>% pull(variable)),  ~is.na(.x))) %>% 
  select(all_of(null_variable %>% pull(variable)),everything())

null_01
```

Not too many. 

For those with null `supervisor_display_name` and `supervisor_e_mail_address`, do they have these names in other parts of the data?

```{r}
null_01 %>%
  filter(if_all(all_of(null_variable %>% pull(variable)),  ~is.na(.x))) %>% 
  select(employee_e_mail_address) %>% 
  distinct() %>% 
  inner_join(prep_base) %>% 
  select(params$unique_identifier_1,all_of(null_variable %>% pull(variable)))
```


### Dataframe of the unique identifier

```{r}
prep_uniq <- 
  prep_base %>% 
  select(params$unique_identifier_1) %>% 
  distinct()

prep_uniq %>% 
  count()
```

### Identifying and categorizing columns

Created a dataframe of variable metadata

```{r}
column_meta_data <- 
  prep_base %>% 
  contents()

column_meta_data <- 
  column_meta_data$contents %>% 
  as.data.frame()

column_meta_data <-
  column_meta_data %>% 
  rownames_to_column(var = "variable")

column_meta_data
```

Function that checks whether there are duplicates for different variables

```{r}
dupe_check <- function(in_data,in_variable_name)
{
  check_01 <- 
  in_data %>% 
  select(params$unique_identifier_1,all_of(in_variable_name)) %>% 
  distinct() %>% 
  get_dupes(params$unique_identifier_1)

out <- nrow(check_01) > 0 

return(out)
}
```

Checking duplicates for each variable in the dataframe.

```{r}
duplicates_by_variable <- 
  column_meta_data %>% 
  pull(variable) %>% 
  map_lgl(~dupe_check(in_data = dupe_01, .))

duplicates_by_variable
```

Add this to the column_meta_data dataframe. 

```{r}
column_meta_data <- 
  column_meta_data %>% 
  add_column(
    duplicates_by_variable = duplicates_by_variable
  )

column_meta_data
```

Create dataframe of only variables that cause duplicates.

```{r}
duplicating_variables <- 
  column_meta_data %>% 
  filter(duplicates_by_variable)

duplicating_variables
```

Grouping variables.

```{r}
duplicating_variables <- 
  duplicating_variables %>% 
  mutate(group = 
           case_when(variable == "short_id" ~ "short_id",
                     variable == "dept_id" ~ "short_id",
                     variable == "supervisor_name" ~ "short_id",
                     variable == "location" ~ "short_id",
                    TRUE ~ variable
                     ))

duplicating_variables
```

Create list of groups.

```{r}
duplicating_groups <- 
  duplicating_variables %>% 
  select(group) %>% 
  distinct() %>% 
  pull()

duplicating_groups
```

### Duplication removal

```{r}
step_n <- 0
```

```{r}
step_n <- step_n + 1
```

#### Step `r step_n`

Removing duplicates produced by:

```{r}
duplicating_group <- 
  duplicating_variables %>% 
  filter(group == duplicating_groups[step_n])
duplicating_group
```

Reviewing duplicates.

```{r}
dupe_01 %>% 
  select(params$unique_identifier_1,all_of(duplicating_group %>% pull(variable))) %>% 
  distinct() %>% 
  get_dupes(params$unique_identifier_1)
```

Not too many duplicates here. 

First count `sso` across the original data.

```{r}
prep_base %>% count(sso)
```

There are just a few, but we'll want to transform these into indicators.

We want to have this change to affect the whole dataframe so we address the duplicates off the original dataframe.

```{r}
prep_xx <- 
  prep_base %>% 
  character_to_indicator(params$unique_identifier_1,sso,"sso")

prep_xx %>% 
  glimpse()

prep_xx %>% 
  skim()
```

Count rows.

```{r}
prep_xx %>% 
  count()
```

Do the number of in this dataframe match the number of unique identifiers?

```{r}
prep_xx %>% 
  count() %>% 
  pull() ==
  prep_uniq %>% 
  count() %>% 
  pull()
```

Rename dataframe.

```{r}
prep_01 <- prep_xx
```

```{r}
step_n <- step_n + 1
```

#### Step `r step_n`

Removing duplicates produced by:

```{r}
duplicating_group <- 
  duplicating_variables %>% 
  filter(group == duplicating_groups[step_n])
duplicating_group
```

Reviewing duplicates.

```{r}
dupe_01 %>% 
  select(params$unique_identifier_1,all_of(duplicating_group %>% pull(variable))) %>% 
  distinct() %>% 
  get_dupes(params$unique_identifier_1)
```

Quite a few duplicates here and they are associated with supervisor and location.

First count `short_id` and `dept_id` across the original data.

```{r}
prep_base %>% count(short_id,dept_id)
```

Counting `short_id`.

```{r}
prep_base %>% count(short_id)
```

Counting `dept_id`.

```{r}
prep_base %>% count(dept_id)
```

Too many options to create useful indicators. 

For lack better option, we'll select the rows with shorter ids.

```{r}
dedupe <- 
  dupe_01 %>% 
  select(params$unique_identifier_1,all_of(duplicating_group %>% pull(variable))) %>% 
  distinct() %>% 
  arrange(!!!params$unique_identifier_1,length(short_id),length(dept_id)) %>% 
  group_by(across(all_of(params$unique_identifier_1))) %>% 
  summarise(
    across(.fns = ~first(.))
  ) 
 
dedupe %>% 
  get_dupes(params$unique_identifier_1)
```

Creating a deduplicated dataframe of `r params$unique_identifier_1` and `r duplicating_groups[step_n]`.

First create a dataframe of distinct rows of the variables from the original data. 

```{r}
working_01 <- 
  prep_base %>% 
  select(params$unique_identifier_1,all_of(duplicating_group %>% pull(variable))) %>% 
  distinct()

working_01 %>% 
  glimpse()

working_01 %>% 
  skim()
```

Next remove rows that dataframe that were found to be duplicated and save that as a new dataframe. 

```{r}
working_02 <- 
  working_01 %>% 
  anti_join(dedupe %>% select(params$unique_identifier_1))

working_02 %>% 
  glimpse()

working_02 %>% 
  skim()
```

Last bind this dataframe to the deduplicated set above.

```{r}
prep_xx <- 
  working_02 %>% 
  bind_rows(dedupe)

prep_xx %>% 
  glimpse()

prep_xx %>% 
  skim()
```

Count rows.

```{r}
prep_xx %>% 
  count()
```

Do the number of in this dataframe match the number of unique identifiers?

```{r}
prep_xx %>% 
  count() %>% 
  pull() ==
  prep_uniq %>% 
  count() %>% 
  pull()
```

Rename dataframe

```{r}
prep_02 <- prep_xx
```


```{r}
step_n <- step_n + 1
```

#### Step `r step_n`

Removing duplicates produced by:

```{r}
duplicating_group <- 
  duplicating_variables %>% 
  filter(group == duplicating_groups[step_n])
duplicating_group
```

Reviewing duplicates.

```{r}
dupe_01 %>% 
  select(params$unique_identifier_1,all_of(duplicating_group %>% pull(variable))) %>% 
  distinct() %>% 
  get_dupes(params$unique_identifier_1)
```


Create function to drop rows with null unique identifiers.

```{r}
filter_unique <- function(in_data, ...)
{
  in_data %>% 
  filter(if_all(c(...),~!is.na(.)))
}
```

Drop rows with null unique identifiers.

```{r}
prep_01 <- 
  prep_01 %>% 
  filter_unique(params$unique_identifier_1)
```

Review data.

```{r}
prep_01 %>% skim()
```

Identify columns with amounts unique values less than 30.

```{r}
low_unique_value_cols <- 
  prep_01 %>% 
  summarise(across(.fns = ~n_distinct(.))) %>% 
  rownames_to_column() %>% 
  pivot_longer(-rowname) %>% 
  filter(value < 30) %>% 
  pull(name)
```

**Unique low value columns**: `r low_unique_value_cols`

Convert low unique value variables to factors.

```{r}
prep_01 <- 
  prep_01 %>% 
  mutate(across(all_of(low_unique_value_cols),as.factor))
```

Review data.

```{r}
prep_01 %>% skim()
```

## Join/bind data {.tabset .tabset-pills}

No data to join or bind so far.

```{r}
d <- prep_01
```

Review data.

```{r}
d %>% skim()
```

## Save data {.tabset .tabset-pills}

```{r}
save(d, file = here::here("03. Data Collection",params$data_folder_1,"prepared_data.rdata"))
```
